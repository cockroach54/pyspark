{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Ch04\").getOrCreate()\n",
    "spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = [\"Elevation\", \"Aspect\", \"Slope\",\n",
    "\"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "\"Horizontal_Distance_To_Roadways\",\n",
    "\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "\"Horizontal_Distance_To_Fire_Points\"]\n",
    "for i in range(4):\n",
    "    colNames += [\"Wilderness_Area_\"+str(i),]\n",
    "for i in range(40):\n",
    "    colNames += [\"Soil_Type_\"+str(i),]\n",
    "colNames += [\"Cover_Type\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType()\n",
    "for name in colNames:\n",
    "    if name == \"Cover_Type\":\n",
    "        schema.add(StructField(name, DoubleType(), True))\n",
    "    else:\n",
    "        schema.add(StructField(name, IntegerType(), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"covtype.data\", header=False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5617"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(0.0.o1)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_0: integer (nullable = true)\n",
      " |-- Wilderness_Area_1: integer (nullable = true)\n",
      " |-- Wilderness_Area_2: integer (nullable = true)\n",
      " |-- Wilderness_Area_3: integer (nullable = true)\n",
      " |-- Soil_Type_0: integer (nullable = true)\n",
      " |-- Soil_Type_1: integer (nullable = true)\n",
      " |-- Soil_Type_2: integer (nullable = true)\n",
      " |-- Soil_Type_3: integer (nullable = true)\n",
      " |-- Soil_Type_4: integer (nullable = true)\n",
      " |-- Soil_Type_5: integer (nullable = true)\n",
      " |-- Soil_Type_6: integer (nullable = true)\n",
      " |-- Soil_Type_7: integer (nullable = true)\n",
      " |-- Soil_Type_8: integer (nullable = true)\n",
      " |-- Soil_Type_9: integer (nullable = true)\n",
      " |-- Soil_Type_10: integer (nullable = true)\n",
      " |-- Soil_Type_11: integer (nullable = true)\n",
      " |-- Soil_Type_12: integer (nullable = true)\n",
      " |-- Soil_Type_13: integer (nullable = true)\n",
      " |-- Soil_Type_14: integer (nullable = true)\n",
      " |-- Soil_Type_15: integer (nullable = true)\n",
      " |-- Soil_Type_16: integer (nullable = true)\n",
      " |-- Soil_Type_17: integer (nullable = true)\n",
      " |-- Soil_Type_18: integer (nullable = true)\n",
      " |-- Soil_Type_19: integer (nullable = true)\n",
      " |-- Soil_Type_20: integer (nullable = true)\n",
      " |-- Soil_Type_21: integer (nullable = true)\n",
      " |-- Soil_Type_22: integer (nullable = true)\n",
      " |-- Soil_Type_23: integer (nullable = true)\n",
      " |-- Soil_Type_24: integer (nullable = true)\n",
      " |-- Soil_Type_25: integer (nullable = true)\n",
      " |-- Soil_Type_26: integer (nullable = true)\n",
      " |-- Soil_Type_27: integer (nullable = true)\n",
      " |-- Soil_Type_28: integer (nullable = true)\n",
      " |-- Soil_Type_29: integer (nullable = true)\n",
      " |-- Soil_Type_30: integer (nullable = true)\n",
      " |-- Soil_Type_31: integer (nullable = true)\n",
      " |-- Soil_Type_32: integer (nullable = true)\n",
      " |-- Soil_Type_33: integer (nullable = true)\n",
      " |-- Soil_Type_34: integer (nullable = true)\n",
      " |-- Soil_Type_35: integer (nullable = true)\n",
      " |-- Soil_Type_36: integer (nullable = true)\n",
      " |-- Soil_Type_37: integer (nullable = true)\n",
      " |-- Soil_Type_38: integer (nullable = true)\n",
      " |-- Soil_Type_39: integer (nullable = true)\n",
      " |-- Cover_Type: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Elevation=3107, Aspect=356, Slope=12, Horizontal_Distance_To_Hydrology=283, Vertical_Distance_To_Hydrology=89, Horizontal_Distance_To_Roadways=4855, Hillshade_9am=201, Hillshade_Noon=219, Hillshade_3pm=155, Horizontal_Distance_To_Fire_Points=2069, Wilderness_Area_0=1, Wilderness_Area_1=0, Wilderness_Area_2=0, Wilderness_Area_3=0, Soil_Type_0=0, Soil_Type_1=0, Soil_Type_2=0, Soil_Type_3=0, Soil_Type_4=0, Soil_Type_5=0, Soil_Type_6=0, Soil_Type_7=0, Soil_Type_8=0, Soil_Type_9=0, Soil_Type_10=0, Soil_Type_11=0, Soil_Type_12=0, Soil_Type_13=0, Soil_Type_14=0, Soil_Type_15=0, Soil_Type_16=0, Soil_Type_17=0, Soil_Type_18=0, Soil_Type_19=0, Soil_Type_20=0, Soil_Type_21=0, Soil_Type_22=0, Soil_Type_23=0, Soil_Type_24=0, Soil_Type_25=0, Soil_Type_26=0, Soil_Type_27=0, Soil_Type_28=1, Soil_Type_29=0, Soil_Type_30=0, Soil_Type_31=0, Soil_Type_32=0, Soil_Type_33=0, Soil_Type_34=0, Soil_Type_35=0, Soil_Type_36=0, Soil_Type_37=0, Soil_Type_38=0, Soil_Type_39=0, Cover_Type=1.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData) = data.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = trainData.drop('Cover_Type').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                           |\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[2061.0,67.0,20.0,228.0,104.0,277.0,236.0,195.0,82.0,450.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,30],[2073.0,119.0,11.0,30.0,-1.0,1276.0,239.0,230.0,120.0,582.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[2085.0,77.0,32.0,150.0,35.0,210.0,240.0,164.0,29.0,474.0,1.0,1.0])     |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[2133.0,292.0,26.0,150.0,70.0,642.0,139.0,225.0,221.0,853.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,30],[2136.0,203.0,20.0,1326.0,206.0,253.0,175.0,1351.0,1.0,1.0])                |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[2140.0,105.0,32.0,60.0,29.0,993.0,254.0,182.0,29.0,342.0,1.0,1.0])     |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[2153.0,116.0,27.0,134.0,75.0,713.0,254.0,203.0,59.0,67.0,1.0,1.0])     |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[2160.0,121.0,25.0,30.0,11.0,175.0,252.0,211.0,71.0,600.0,1.0,1.0])     |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,17],[2172.0,128.0,13.0,1211.0,241.0,232.0,118.0,1084.0,1.0,1.0])                |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[2202.0,129.0,30.0,201.0,102.0,767.0,252.0,207.0,59.0,182.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,27],[2221.0,318.0,21.0,30.0,12.0,834.0,161.0,216.0,194.0,309.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[2226.0,186.0,23.0,30.0,11.0,1110.0,217.0,249.0,153.0,342.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[2227.0,334.0,28.0,270.0,90.0,1045.0,145.0,190.0,178.0,842.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[2228.0,122.0,30.0,90.0,31.0,785.0,254.0,201.0,52.0,1506.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[2229.0,130.0,15.0,30.0,8.0,592.0,243.0,230.0,112.0,1725.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[2249.0,6.0,27.0,319.0,165.0,420.0,175.0,175.0,127.0,573.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[2249.0,334.0,33.0,277.0,147.0,1198.0,127.0,177.0,178.0,1136.0,1.0,1.0])|\n",
      "|(54,[0,1,2,5,6,7,8,9,13,17],[2259.0,24.0,3.0,752.0,218.0,233.0,152.0,607.0,1.0,1.0])                    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[2261.0,339.0,18.0,85.0,22.0,1392.0,179.0,212.0,170.0,454.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[2271.0,13.0,20.0,376.0,113.0,1400.0,196.0,195.0,131.0,859.0,1.0,1.0])  |\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=inputCols,\n",
    "    outputCol=\"featureVector\")\n",
    "assembledTrainData = assembler.transform(trainData)\n",
    "assembledTrainData.select('featureVector').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4765952a6c4a374ac222) of depth 5 with 61 nodes\n",
      "  If (feature 0 <= 3057.5)\n",
      "   If (feature 0 <= 2567.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 23 <= 0.5)\n",
      "      If (feature 24 <= 0.5)\n",
      "       Predict: 3.0\n",
      "      Else (feature 24 > 0.5)\n",
      "       Predict: 2.0\n",
      "     Else (feature 23 > 0.5)\n",
      "      If (feature 4 <= 108.5)\n",
      "       Predict: 6.0\n",
      "      Else (feature 4 > 108.5)\n",
      "       Predict: 3.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     If (feature 9 <= 5365.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 9 > 5365.5)\n",
      "      If (feature 1 <= 88.5)\n",
      "       Predict: 5.0\n",
      "      Else (feature 1 > 88.5)\n",
      "       Predict: 2.0\n",
      "   Else (feature 0 > 2567.5)\n",
      "    If (feature 0 <= 2947.5)\n",
      "     If (feature 12 <= 0.5)\n",
      "      If (feature 5 <= 448.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 448.0)\n",
      "       Predict: 2.0\n",
      "     Else (feature 12 > 0.5)\n",
      "      If (feature 0 <= 2742.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 2742.5)\n",
      "       Predict: 2.0\n",
      "    Else (feature 0 > 2947.5)\n",
      "     If (feature 3 <= 101.5)\n",
      "      If (feature 35 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 35 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 3 > 101.5)\n",
      "      If (feature 7 <= 238.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 7 > 238.5)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3057.5)\n",
      "   If (feature 0 <= 3315.5)\n",
      "    If (feature 3 <= 231.0)\n",
      "     If (feature 0 <= 3191.5)\n",
      "      If (feature 45 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 45 > 0.5)\n",
      "       Predict: 2.0\n",
      "     Else (feature 0 > 3191.5)\n",
      "      If (feature 8 <= 64.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 8 > 64.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 3 > 231.0)\n",
      "     If (feature 7 <= 238.5)\n",
      "      If (feature 0 <= 3100.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3100.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 7 > 238.5)\n",
      "      If (feature 0 <= 3208.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3208.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3315.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 5 <= 3843.0)\n",
      "      If (feature 52 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 52 > 0.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 5 > 3843.0)\n",
      "      If (feature 9 <= 2956.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 9 > 2956.5)\n",
      "       Predict: 7.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     If (feature 9 <= 3237.5)\n",
      "      If (feature 48 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 48 > 0.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 9 > 3237.5)\n",
      "      If (feature 1 <= 79.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 1 > 79.5)\n",
      "       Predict: 1.0\n",
      "\n",
      "(54,[0,1,3,4,5,7,8,9,10,12,23,24,35,45,48,52],[0.7656586505774275,0.002700700342779686,0.04931194289494332,0.008807077285966518,0.019586610511832733,0.03761903039671427,0.004232008086627663,0.009387982878320005,0.05032732572195084,0.0140646039451027,0.008762256652577797,0.004950189639472009,0.007204744823875496,0.0058948835654659216,0.0032412283593845847,0.00825076431755882])\n",
      "Wall time: 6.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(labelCol=\"Cover_Type\", featuresCol=\"featureVector\", predictionCol=\"prediction\")\n",
    "model = classifier.fit(assembledTrainData)\n",
    "\n",
    "print(model.toDebugString)\n",
    "print(model.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "# svm = LinearSVC(maxIter=5, regParam=0.01)\n",
    "# model = svm.fit(assembledTrainData)\n",
    "\n",
    "logi = LogisticRegression(labelCol=\"Cover_Type\", featuresCol=\"featureVector\", family=\"multinomial\", predictionCol=\"prediction\")\n",
    "model = logi.fit(assembledTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                                                                             |\n",
      "+----------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|3.0       |3.0       |[2.76290108001562E-7,1.3086626661125166E-6,3.5776642178765396E-5,0.8177268862982532,0.04050058971453184,8.010180038988374E-5,0.14165438897268393,6.716191883479357E-7]  |\n",
      "|4.0       |4.0       |[2.077645783782002E-7,1.898473048548013E-8,0.0010355683728960094,0.34682323647133984,0.5706282248479218,0.0010972423656961945,0.0804154854501259,1.5742711540045418E-8] |\n",
      "|6.0       |3.0       |[4.781132851442249E-7,9.224253214029815E-6,0.006152198657796492,0.8934778696615581,0.002032756372054227,1.2736437309016018E-4,0.09819992523854654,1.833304551499196E-7] |\n",
      "|6.0       |3.0       |[5.679792612216657E-7,4.7243229653591305E-5,0.06895182173027127,0.6002764460756879,7.310556382345063E-5,4.90482184695394E-4,0.33016021207875673,1.2115785049717238E-7]  |\n",
      "|4.0       |3.0       |[2.2181258475515208E-7,2.832487553386829E-8,0.002135922149465582,0.6152101654232254,0.23615623905577143,0.0021744434615968604,0.14432297330388333,6.4685970599889285E-9]|\n",
      "|4.0       |4.0       |[2.120111375349465E-7,3.194420957024841E-7,0.0025609583542837286,0.3387195169290149,0.6484583653173294,5.500948102213964E-6,0.010254945434267764,1.8156376885059636E-7] |\n",
      "|4.0       |3.0       |[2.0709592640342314E-7,2.2714094108857747E-7,0.003801075615685,0.5024298893101731,0.4769251093068124,1.049177467324198E-5,0.016832911247959628,8.850782911447314E-8]    |\n",
      "|3.0       |3.0       |[1.7910995667735768E-7,9.182114042429874E-7,1.433861923512515E-5,0.814483227074031,0.048406187008345085,7.361089172812642E-5,0.1370212911145171,2.479707826577838E-7]   |\n",
      "|4.0       |3.0       |[4.81896810118619E-7,6.713192063148026E-7,0.0059228266346758535,0.6289079472265636,0.3482364930432384,4.1515505776360716E-4,0.01651601394274965,4.1087899242642855E-7]  |\n",
      "|4.0       |3.0       |[1.9866013032238866E-7,2.644010770469291E-7,0.003628319434206946,0.5878047272862885,0.3929421619701561,1.233816090965565E-5,0.01561191543845114,7.464878008082523E-8]   |\n",
      "|3.0       |6.0       |[2.616334334556806E-8,3.874103576881222E-7,1.7560466954889903E-6,0.45719418176265103,8.92687559456036E-6,8.737248289437146E-6,0.5427858980218189,8.647124970684106E-8]  |\n",
      "|4.0       |3.0       |[1.723219590023599E-7,2.136210705464247E-6,4.818467369035915E-5,0.789509960640126,0.013692012661887491,9.567997519602716E-5,0.19665171377093438,1.3974550136278421E-7]  |\n",
      "|6.0       |3.0       |[1.6105618795059628E-6,2.7936198053255874E-4,0.10790704672824249,0.5514802972710888,1.65980525690317E-4,8.537557046394397E-4,0.3393096165139729,2.3307139541792843E-6]  |\n",
      "|4.0       |4.0       |[3.3155288163835547E-7,7.922390937077873E-7,0.005908707085554169,0.4044325998115399,0.5694530193858571,1.1597457645091515E-5,0.020192556325153547,3.961422749097634E-7] |\n",
      "|4.0       |4.0       |[7.123127255261945E-7,2.8491150311424345E-6,0.026466276936321437,0.39089994051572124,0.5155836727403122,2.8987879937633428E-5,0.06701612440443669,1.4360955141617686E-6]|\n",
      "|3.0       |3.0       |[1.9481372888948172E-6,4.129998340825557E-4,0.10848841553746853,0.6932208254463664,5.098963475194934E-4,3.277261912116775E-4,0.19703329426823737,4.894237825153525E-6]  |\n",
      "|3.0       |3.0       |[2.3644375761531267E-6,6.233979103917804E-4,0.1261523395576062,0.512062260287038,1.5368414472429387E-4,0.001088814028664444,0.35991006680960635,7.072824392698622E-6]   |\n",
      "|4.0       |3.0       |[1.6763018596242898E-6,1.368199928962751E-5,0.029126256817494676,0.5479765642147146,0.3942736234907935,6.625168758845372E-4,0.02793830276291891,7.377537044438783E-6]   |\n",
      "|6.0       |6.0       |[1.4907667414829684E-6,2.1189319105147333E-4,0.07661981862640325,0.4166407094762627,3.5641650374079183E-4,8.987530183588873E-4,0.5052684045481685,2.5138692729506747E-6]|\n",
      "|3.0       |3.0       |[1.8473529082836768E-6,4.6545394193762334E-4,0.20409206772239252,0.5941462388381492,5.912136744392341E-4,2.477683487152592E-4,0.2004503290433967,5.08107806114829E-6]   |\n",
      "+----------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Wall time: 431 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = model.transform(assembledTrainData)\n",
    "predictions.select([\"Cover_Type\", \"prediction\", \"probability\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218712988551125"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7116944546332543"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### confusion matrix - not supported in PySpark ML library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "inputCols = trainData.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol=\"featureVector\")\n",
    "classifier = DecisionTreeClassifier(labelCol=\"Cover_Type\", featuresCol=\"featureVector\", predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(classifier.impurity, [\"gini\", \"entropy\"])\\\n",
    "    .addGrid(classifier.maxBins, [40, 300])\\\n",
    "    .addGrid(classifier.minInfoGain, [0.0, 0.05])\\\n",
    "    .build()\n",
    "\n",
    "#     .addGrid(classifier.numTrees, [1, 20])\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "multiclassEval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Cover_Type\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")\n",
    "multiclassEval.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "validator = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=multiclassEval,\n",
    "    trainRatio=0.9)\n",
    "\n",
    "validatorModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = validatorModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_463391926e94708693ad,\n",
       " VectorIndexer_4badbdce6fdd84798ae0,\n",
       " RandomForestClassificationModel (uid=RandomForestClassifier_4a15a54a4df658497bc8) with 1 trees]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='featuresCol', doc='features column name'): 'featureVector',\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='labelCol', doc='label column name'): 'Cover_Type',\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='DecisionTreeClassifier_44298458fca7d1522d02', name='seed', doc='random seed'): -900016594924757190}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7480769230769231,\n",
       " 0.7115384615384616,\n",
       " 0.7269230769230769,\n",
       " 0.7115384615384616,\n",
       " 0.7288461538461538,\n",
       " 0.7192307692307692,\n",
       " 0.7269230769230769,\n",
       " 0.7192307692307692]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsAndMetrics = validatorModel.validationMetrics\n",
    "paramsAndMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.720508166969147"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclassEval.evaluate(bestModel.transform(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### undoing the one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildernessCols = []\n",
    "for i in range(4):\n",
    "    wildernessCols += [\"Wilderness_Area_\"+str(i),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildernessAssembler = VectorAssembler(\n",
    "    inputCols=wildernessCols,\n",
    "    outputCol=\"wilderness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StructType\n",
    "\n",
    "unhotudf = udf(lambda x: float(x.toArray().nonzero()[0]), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Elevation=3107, Aspect=356, Slope=12, Horizontal_Distance_To_Hydrology=283, Vertical_Distance_To_Hydrology=89, Horizontal_Distance_To_Roadways=4855, Hillshade_9am=201, Hillshade_Noon=219, Hillshade_3pm=155, Horizontal_Distance_To_Fire_Points=2069, Soil_Type_0=0, Soil_Type_1=0, Soil_Type_2=0, Soil_Type_3=0, Soil_Type_4=0, Soil_Type_5=0, Soil_Type_6=0, Soil_Type_7=0, Soil_Type_8=0, Soil_Type_9=0, Soil_Type_10=0, Soil_Type_11=0, Soil_Type_12=0, Soil_Type_13=0, Soil_Type_14=0, Soil_Type_15=0, Soil_Type_16=0, Soil_Type_17=0, Soil_Type_18=0, Soil_Type_19=0, Soil_Type_20=0, Soil_Type_21=0, Soil_Type_22=0, Soil_Type_23=0, Soil_Type_24=0, Soil_Type_25=0, Soil_Type_26=0, Soil_Type_27=0, Soil_Type_28=1, Soil_Type_29=0, Soil_Type_30=0, Soil_Type_31=0, Soil_Type_32=0, Soil_Type_33=0, Soil_Type_34=0, Soil_Type_35=0, Soil_Type_36=0, Soil_Type_37=0, Soil_Type_38=0, Soil_Type_39=0, Cover_Type=1.0, wilderness=0.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withWilderness = wildernessAssembler.transform(data)\n",
    "withWilderness = withWilderness\\\n",
    "    .drop(*wildernessCols)\\\n",
    "    .withColumn(\"wilderness\", unhotudf(withWilderness['wilderness']))\n",
    "withWilderness.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soilCols = []\n",
    "for i in range(40):\n",
    "    soilCols += [\"Soil_Type_\"+str(i),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "soilAssembler = VectorAssembler(\n",
    "        inputCols=soilCols,\n",
    "        outputCol=\"soil\")\n",
    "\n",
    "withWilderness = soilAssembler.transform(withWilderness)\n",
    "unencData = withWilderness\\\n",
    "    .drop(*soilCols)\\\n",
    "    .withColumn(\"soil\", unhotudf(withWilderness['soil']))\n",
    "unencData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier with unencoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unencTrainData, unencTestData) = unencData.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "inputCols = unencTrainData.drop('Cover_Type').columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=inputCols,\n",
    "    outputCol=\"featureVector\")\n",
    "indexer = VectorIndexer(\n",
    "    maxCategories=40,\n",
    "    inputCol=\"featureVector\",\n",
    "    outputCol=\"indexedVector\")\n",
    "classifier = DecisionTreeClassifier(\n",
    "    seed=42,\n",
    "    labelCol=\"Cover_Type\",\n",
    "    featuresCol=\"indexedVector\",\n",
    "    predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, indexer, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(\n",
    "    seed=42,\n",
    "    maxBins=40,.\n",
    "    labelCol=\"Cover_Type\",\n",
    "    featuresCol=\".\",\n",
    "    predictionCol=\"prediction\"k)\n",
    "pipeline = Pipeline(stages=[assembler, indexer, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(classifier.minInfoGain, [0.0, 0.05])\\\n",
    "    .addGrid(classifier.numTrees, [1, 10])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclassEval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Cover_Type\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = TrainValidationSplit(\n",
    "    seed=42,\n",
    "    estimator=pipeline,\n",
    "    evaluator=multiclassEval,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto', Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='featuresCol', doc='features column name'): 'indexedVector', Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini', Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='labelCol', doc='label column name'): 'Cover_Type', Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='numTrees', doc='Number of trees to train (>= 1)'): 1, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='seed', doc='random seed'): 42, Param(parent='RandomForestClassifier_4a15a54a4df658497bc8', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validatorModel = validator.fit(unencTrainData)\n",
    "bestModel = validatorModel.bestModel\n",
    "forestModel = bestModel.stages[-1]\n",
    "print(forestModel.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forestModel.getNumTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elevation', 0.7490484478495304),\n",
       " ('soil', 0.14747148426805073),\n",
       " ('Horizontal_Distance_To_Roadways', 0.028555255238566488),\n",
       " ('Horizontal_Distance_To_Hydrology', 0.027207391866001514),\n",
       " ('Hillshade_Noon', 0.018159766360003802),\n",
       " ('Hillshade_9am', 0.012692986886932833),\n",
       " ('wilderness', 0.008479764416715177),\n",
       " ('Horizontal_Distance_To_Fire_Points', 0.004904580279106281),\n",
       " ('Slope', 0.0034803228350927216),\n",
       " ('Aspect', 0.0),\n",
       " ('Vertical_Distance_To_Hydrology', 0.0),\n",
       " ('Hillshade_3pm', 0.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(inputCols, forestModel.featureImportances)), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7046728971962617"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAccuracy = multiclassEval.evaluate(bestModel.transform(unencTestData))\n",
    "testAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestModel.transform(unencTestData.drop(\"Cover_Type\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
