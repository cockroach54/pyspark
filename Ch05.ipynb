{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Anomaly Detection in Network Traffic with K-means Clustering\n",
    "http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-AKJ0BC3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kmeans</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=kmeans>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pprint import pprint\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"kmeans\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\").appName(\"kmeans\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A First Take on Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWithoutHeader = spark.read.option('inferSchema', 'true') \\\n",
    "                            .option('header', 'false') \\\n",
    "                            .csv('kddcup.data_10_percent_corrected')\n",
    "# 10% sampling dataser --> 500k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataWithoutHeader.toDF(\n",
    "\"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "\"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "\"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "\"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "\"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "\"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "\"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "\"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "\"dst_host_count\", \"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "\"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "\"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"label\").groupBy(\"label\").count().orderBy(\"count\", ascending=False).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").dropna().cache()\n",
    "numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = numericOnly.columns\n",
    "assembler = VectorAssembler() \\\n",
    "    .setInputCols(cols.remove(\"label\"))\\\n",
    "    .setInputCols([\"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"])\\\n",
    "    .setOutputCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans() \\\n",
    "    .setPredictionCol(\"cluster\") \\\n",
    "    .setFeaturesCol(\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00190743, 0.00097509]), array([0.9805854 , 0.98359484])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline().setStages([assembler, kmeans]) # stages가 뭐임 -> transformer, estimator\n",
    "# pipeline = Pipeline(stages=[assembler, kmeans])\n",
    "pipelineModel = pipeline.fit(numericOnly)\n",
    "kmeansModel = pipelineModel.stages[-1]\n",
    "\n",
    "kmeansModel.clusterCenters() # k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "withCluster = pipelineModel.transform(numericOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|          smurf.|280790|\n",
      "|      0|         normal.| 91744|\n",
      "|      0|        neptune.| 86744|\n",
      "|      0|           back.|  2191|\n",
      "|      0|        ipsweep.|  1160|\n",
      "|      0|    warezclient.|  1020|\n",
      "|      0|       teardrop.|   979|\n",
      "|      0|          satan.|   359|\n",
      "|      0|            pod.|   264|\n",
      "|      0|           nmap.|   231|\n",
      "|      0|      portsweep.|    37|\n",
      "|      0|buffer_overflow.|    30|\n",
      "|      0|           land.|    21|\n",
      "|      0|    warezmaster.|    20|\n",
      "|      0|           imap.|    12|\n",
      "|      0|        rootkit.|    10|\n",
      "|      0|     loadmodule.|     9|\n",
      "|      0|      ftp_write.|     8|\n",
      "|      0|       multihop.|     7|\n",
      "|      0|            phf.|     4|\n",
      "|      0|           perl.|     3|\n",
      "|      0|   guess_passwd.|     2|\n",
      "|      0|            spy.|     2|\n",
      "|      1|        neptune.| 20457|\n",
      "|      1|         normal.|  5534|\n",
      "+-------+----------------+------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withCluster.select(\"cluster\", \"label\") \\\n",
    "    .groupBy(\"cluster\", \"label\").count() \\\n",
    "    .orderBy([\"cluster\", \"count\"], ascending=[1, 0]) \\\n",
    "    .show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def clusteringScore0(data, k): # (data: DataFrame, k: Int): Double \n",
    "    cols = data.columns.copy()\n",
    "    cols.remove(\"label\")\n",
    "\n",
    "    assembler = VectorAssembler() \\\n",
    "        .setInputCols(cols) \\\n",
    "        .setOutputCol(\"featureVector\")\n",
    "        \n",
    "    kmeans = KMeans() \\\n",
    "        .setSeed(random.randint(0,1000)) \\\n",
    "        .setK(k) \\\n",
    "        .setPredictionCol(\"cluster\") \\\n",
    "        .setFeaturesCol(\"featureVector\")\n",
    "    \n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 33186397.422698718),\n",
       " (40, 33483863.119649567),\n",
       " (60, 34134989.49093766),\n",
       " (80, 15127527.320445474),\n",
       " (100, 14357179.118356757)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores0 = map(lambda x: (x, clusteringScore0(numericOnly, x)) ,range(20, 101, 20))\n",
    "list(scores0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore1(data, k): # (data: DataFrame, k: Int): Double \n",
    "    cols = data.columns.copy()\n",
    "    cols.remove(\"label\")\n",
    "\n",
    "    assembler = VectorAssembler() \\\n",
    "        .setInputCols(cols) \\\n",
    "        .setOutputCol(\"featureVector\")\n",
    "        \n",
    "    kmeans = KMeans() \\\n",
    "        .setSeed(random.randint(0,1000)) \\\n",
    "        .setK(k) \\\n",
    "        .setMaxIter(40) \\\n",
    "        .setTol(1.0e-5) \\\n",
    "        .setPredictionCol(\"cluster\") \\\n",
    "        .setFeaturesCol(\"featureVector\")\n",
    "    \n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 34478047.18040431),\n",
       " (40, 34001097.514237694),\n",
       " (60, 23406981.74675835),\n",
       " (80, 25109958.64245254),\n",
       " (100, 3628639.6452119444)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = map(lambda x: (x, clusteringScore1(numericOnly, x)) ,range(20, 101, 20))\n",
    "list(scores1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with SparkR\n",
    "> R code는 내용이 같으므로 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def clusteringScore2(data, k): #def clusteringScore2(data: DataFrame, k: Int): Double = {\n",
    "    cols = data.columns.copy()\n",
    "    cols.remove(\"label\")\n",
    "\n",
    "    assembler = VectorAssembler() \\\n",
    "        .setInputCols(cols) \\\n",
    "        .setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler() \\\n",
    "        .setInputCol(\"featureVector\") \\\n",
    "        .setOutputCol(\"scaledFeatureVector\") \\\n",
    "        .setWithStd(True) \\\n",
    "        .setWithMean(False)\n",
    "        \n",
    "    kmeans = KMeans() \\\n",
    "        .setSeed(random.randint(0,1000)) \\\n",
    "        .setK(k) \\\n",
    "        .setMaxIter(40) \\\n",
    "        .setTol(1.0e-5) \\\n",
    "        .setPredictionCol(\"cluster\") \\\n",
    "        .setFeaturesCol(\"featureVector\")\n",
    "    \n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 3940231.1333903004),\n",
       " (90, 34130705.46761614),\n",
       " (120, 5750760.496748073),\n",
       " (150, 2957923.03802364),\n",
       " (180, 1065087.2376975007),\n",
       " (210, 1760915.8189446821),\n",
       " (240, 9760382.727434369),\n",
       " (270, 6260249.827110213)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = map(lambda x: (x, clusteringScore2(numericOnly, x)) ,range(60, 271, 30))\n",
    "list(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "def oneHotPipeline(inputCol): # (inputCol: String): (Pipeline, String)\n",
    "    indexer = StringIndexer(inputCol=inputCol, outputCol=inputCol+\"_indexed\")   \n",
    "    encoder = OneHotEncoder(inputCol=inputCol+\"_indexed\", outputCol=inputCol+\"_vec\")\n",
    "\n",
    "    pipeline = Pipeline().setStages([indexer, encoder])\n",
    "    return (pipeline, inputCol + \"_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot_df(data):\n",
    "    (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
    "    (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
    "    (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
    "    \n",
    "    pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder])\n",
    "    return pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore3(data, k): # data: DataFrame, k: Int): Double = {\n",
    "       \n",
    "    cols = data.columns.copy()\n",
    "    for c in [\"protocol_type\", \"service\", \"flag\", \"protocol_type_indexed\", \"service_indexed\", \"flag_indexed\", \"label\"]:\n",
    "        cols.remove(c)\n",
    "    cols.extend([\"protocol_type_vec\", \"service_vec\", \"flag_vec\"])\n",
    "\n",
    "    assembler = VectorAssembler() \\\n",
    "        .setInputCols(cols) \\\n",
    "        .setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler() \\\n",
    "        .setInputCol(\"featureVector\") \\\n",
    "        .setOutputCol(\"scaledFeatureVector\") \\\n",
    "        .setWithStd(True) \\\n",
    "        .setWithMean(False)\n",
    "        \n",
    "    kmeans = KMeans() \\\n",
    "        .setSeed(random.randint(0,1000)) \\\n",
    "        .setK(k) \\\n",
    "        .setMaxIter(40) \\\n",
    "        .setTol(1.0e-5) \\\n",
    "        .setPredictionCol(\"cluster\") \\\n",
    "        .setFeaturesCol(\"featureVector\")\n",
    "    \n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data)) / data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 26934794.934924006),\n",
       " (90, 1356914.6988293286),\n",
       " (120, 3270926.8840265777),\n",
       " (150, 4101677.2447230327),\n",
       " (180, 2953135.322722774),\n",
       " (210, 1477482.3868878032),\n",
       " (240, 5576328.503406511),\n",
       " (270, 1962455.3764015127)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_one_hot = make_one_hot_df(data)\n",
    "scores3 = map(lambda x: (x, clusteringScore3(data_one_hot, x)), range(60, 271, 30))\n",
    "list(scores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Labels with Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#  Calc entropy\n",
    "#  파이썬 map은 제너레이터\n",
    "def entropy(counts): # (counts: iterable[int]): Double\n",
    "    values = counts.filter(lambda x: x>0)\n",
    "    n = values.map(float).sum()\n",
    "    entropys = values.map(lambda v: calc_each_entropy(v,n))\n",
    "    return entropys.sum()\n",
    "\n",
    "def calc_each_entropy(v, n):\n",
    "    p = v/n\n",
    "    return -p*math.log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[465647, 28374]\n"
     ]
    }
   ],
   "source": [
    "clusterLabel = pipelineModel.transform(data).select([\"cluster\", \"label\"])\n",
    "labels_grouped = clusterLabel.rdd.groupByKey()\n",
    "labels_size = labels_grouped.mapValues(len).map(lambda x: x[1]).collect()\n",
    "print(labels_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475792.1616625872, 23870.055042170145]\n"
     ]
    }
   ],
   "source": [
    "# RDD.map안에서 다시 sc.parallelize 선언시 에러남\n",
    "labels_count = labels_grouped.map(lambda x: Counter(x[1].data)).collect()\n",
    "labels_count_list = list(map(lambda x: sc.parallelize(x.values()), labels_count))\n",
    "weightedClusterEntropy = list(map(lambda x,y: x*entropy(y), labels_size, labels_count_list))\n",
    "print(weightedClusterEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0114189815913845"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(weightedClusterEntropy)/data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitPipeline4(data, k): # (data: DataFrame, k: Int): PipelineModel\n",
    "    \n",
    "    cols = data.columns.copy()\n",
    "    for c in [\"protocol_type\", \"service\", \"flag\", \"protocol_type_indexed\", \"service_indexed\", \"flag_indexed\", \"label\"]:\n",
    "        cols.remove(c)\n",
    "    cols.extend([\"protocol_type_vec\", \"service_vec\", \"flag_vec\"])\n",
    "\n",
    "    assembler = VectorAssembler() \\\n",
    "        .setInputCols(cols) \\\n",
    "        .setOutputCol(\"featureVector\")\n",
    "    \n",
    "    scaler = StandardScaler() \\\n",
    "        .setInputCol(\"featureVector\") \\\n",
    "        .setOutputCol(\"scaledFeatureVector\") \\\n",
    "        .setWithStd(True) \\\n",
    "        .setWithMean(False)\n",
    "        \n",
    "    kmeans = KMeans() \\\n",
    "        .setSeed(random.randint(0,1000)) \\\n",
    "        .setK(k) \\\n",
    "        .setMaxIter(40) \\\n",
    "        .setTol(1.0e-5) \\\n",
    "        .setPredictionCol(\"cluster\") \\\n",
    "        .setFeaturesCol(\"featureVector\")\n",
    "    \n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    return pipeline.fit(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore4(data, k): # (data: DataFrame, k: Int): Double \n",
    "    pipelineModel = fitPipeline4(data, k)\n",
    "\n",
    "    # Predict cluster for each datum\n",
    "    clusterLabel = pipelineModel.transform(data).select([\"cluster\", \"label\"])\n",
    "    labels_grouped = clusterLabel.rdd.groupByKey()\n",
    "    labels_size = labels_grouped.mapValues(len).map(lambda x: x[1]).collect()\n",
    "\n",
    "    # Extract collections of labels, per cluster\n",
    "    labels_count = labels_grouped.map(lambda x: Counter(x[1].data)).collect()\n",
    "    labels_count_list = list(map(lambda x: sc.parallelize(x.values()), labels_count))\n",
    "    weightedClusterEntropy = list(map(lambda x,y: x*entropy(y), labels_size, labels_count_list))\n",
    "\n",
    "\n",
    "    # Average entropy weighted by cluster size    \n",
    "    return sum(weightedClusterEntropy)/data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_load_from_socket\u001b[1;34m(sock_info, serializer)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-ac4fcce71653>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscores4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusteringScore4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m271\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-192-ac4fcce71653>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusteringScore4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m271\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-191-201fc9b6cd22>\u001b[0m in \u001b[0;36mclusteringScore4\u001b[1;34m(data, k)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlabels_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels_grouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlabels_count_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mweightedClusterEntropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_count_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-191-201fc9b6cd22>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlabels_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels_grouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlabels_count_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mweightedClusterEntropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_count_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-842395b79e4e>\u001b[0m in \u001b[0;36mentropy\u001b[1;34m(counts)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# (counts: iterable[int]): Double\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mentropys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcalc_each_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mentropys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1062\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \"\"\"\n\u001b[1;32m-> 1064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    933\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_load_from_socket\u001b[1;34m(sock_info, serializer)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 매우 오래 걸림 노트북에서 안하는걸 추천\n",
    "# 혹은 데이터 샘플링 추천\n",
    "scores4 = map(lambda x: (x, clusteringScore4(data_one_hot, x)), range(60, 271, 30))\n",
    "list(scores4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0784599434573783"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusteringScore4(data_one_hot, 20) # 너무 오래거려서 k=20인 경우만 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|           back.|     4|\n",
      "|      0|buffer_overflow.|    21|\n",
      "|      0|      ftp_write.|     7|\n",
      "|      0|   guess_passwd.|    53|\n",
      "|      0|           imap.|    10|\n",
      "|      0|        ipsweep.|  1247|\n",
      "|      0|           land.|    21|\n",
      "|      0|     loadmodule.|     6|\n",
      "|      0|       multihop.|     3|\n",
      "|      0|        neptune.|107201|\n",
      "|      0|           nmap.|   231|\n",
      "|      0|         normal.| 84249|\n",
      "|      0|           perl.|     3|\n",
      "|      0|            pod.|   264|\n",
      "|      0|      portsweep.|  1039|\n",
      "|      0|        rootkit.|     7|\n",
      "|      0|          satan.|  1589|\n",
      "|      0|          smurf.|280790|\n",
      "|      0|            spy.|     2|\n",
      "|      0|       teardrop.|   979|\n",
      "|      0|    warezclient.|   960|\n",
      "|      0|    warezmaster.|     3|\n",
      "|      1|      portsweep.|     1|\n",
      "|      2|    warezclient.|    59|\n",
      "|      3|         normal.|     1|\n",
      "|      3|    warezmaster.|    15|\n",
      "|      4|         normal.|     3|\n",
      "|      5|           back.|  2190|\n",
      "|      5|         normal.|    89|\n",
      "|      6|         normal.|     1|\n",
      "|      7|         normal.|     4|\n",
      "|      8|         normal.|     2|\n",
      "|      9|         normal.|     1|\n",
      "|     10|buffer_overflow.|     1|\n",
      "|     10|         normal.|   125|\n",
      "|     11|      ftp_write.|     1|\n",
      "|     11|       multihop.|     1|\n",
      "|     11|         normal.|  1926|\n",
      "|     11|        rootkit.|     1|\n",
      "|     12|         normal.|     1|\n",
      "|     13|       multihop.|     1|\n",
      "|     13|         normal.|     8|\n",
      "|     14|           back.|     9|\n",
      "|     14|buffer_overflow.|     8|\n",
      "|     14|           imap.|     1|\n",
      "|     14|     loadmodule.|     3|\n",
      "|     14|       multihop.|     1|\n",
      "|     14|         normal.| 10697|\n",
      "|     14|            phf.|     4|\n",
      "|     14|        rootkit.|     2|\n",
      "|     14|    warezmaster.|     1|\n",
      "|     15|         normal.|     1|\n",
      "|     16|         normal.|     3|\n",
      "|     17|         normal.|     6|\n",
      "|     18|         normal.|     1|\n",
      "|     19|         normal.|    21|\n",
      "|     20|         normal.|     1|\n",
      "|     21|         normal.|     1|\n",
      "|     22|    warezclient.|     1|\n",
      "|     23|       multihop.|     1|\n",
      "|     23|         normal.|     1|\n",
      "|     24|         normal.|    12|\n",
      "|     25|         normal.|     1|\n",
      "|     26|         normal.|     1|\n",
      "|     27|         normal.|     1|\n",
      "|     28|         normal.|     1|\n",
      "|     29|         normal.|     1|\n",
      "|     30|         normal.|     3|\n",
      "|     31|         normal.|    45|\n",
      "|     32|         normal.|     1|\n",
      "|     33|         normal.|     1|\n",
      "|     34|         normal.|     1|\n",
      "|     35|         normal.|     1|\n",
      "|     36|         normal.|     1|\n",
      "|     37|         normal.|     1|\n",
      "|     38|         normal.|     1|\n",
      "|     39|         normal.|     1|\n",
      "|     40|         normal.|     1|\n",
      "|     41|         normal.|     1|\n",
      "|     41|    warezmaster.|     1|\n",
      "|     42|         normal.|     1|\n",
      "|     43|         normal.|     1|\n",
      "|     44|         normal.|    35|\n",
      "|     45|           imap.|     1|\n",
      "|     45|         normal.|     4|\n",
      "|     46|         normal.|     2|\n",
      "|     47|         normal.|     1|\n",
      "|     48|         normal.|     2|\n",
      "|     49|         normal.|     1|\n",
      "|     50|         normal.|     1|\n",
      "|     51|         normal.|     2|\n",
      "|     52|         normal.|     2|\n",
      "|     53|         normal.|     1|\n",
      "|     54|         normal.|     2|\n",
      "|     55|         normal.|     1|\n",
      "|     56|         normal.|     1|\n",
      "|     57|         normal.|     1|\n",
      "|     58|         normal.|     2|\n",
      "|     59|         normal.|     1|\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = fitPipeline4(data_one_hot, 60) # 결과가 모두 다를거고 여기선 k=60이 최적이라는 가정하에 계산\n",
    "countByClusterLabel = pipelineModel.transform(data_one_hot) \\\n",
    "    .select(\"cluster\", \"label\") \\\n",
    "    .groupBy(\"cluster\", \"label\").count() \\\n",
    "    .orderBy([\"cluster\", \"label\"])\n",
    "countByClusterLabel.show(200)\n",
    "\n",
    "# 전체 데이터를 다 쓰지 않아서인지 결과가 좋지 않습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "kMeansModel = pipelineModel.stages[-1]\n",
    "centroids = kMeansModel.clusterCenters()\n",
    "clustered = pipelineModel.transform(data_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855.0507798357681"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sqdist(a,b):  return float(np.sqrt(np.sum((a-b)**2, axis=0)))\n",
    "    \n",
    "thresholds = clustered.select(\"cluster\", \"scaledFeatureVector\").rdd \\\n",
    "    .map(lambda x: sqdist(centroids[x[0]], np.array(x[1]))).collect()\n",
    "threshold = sorted(thresholds)[50]\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = clustered.sample(0.01) # 너무 오래걸려서  1%만 샘플링\n",
    "samples.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = samples.select(\"cluster\", \"scaledFeatureVector\", \"label\").rdd \\\n",
    "    .filter(lambda x: sqdist(centroids[x[0]], np.array(x[1])) >= threshold).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|           label|count|\n",
      "+----------------+-----+\n",
      "|          smurf.| 2856|\n",
      "|        neptune.| 1097|\n",
      "|         normal.|  995|\n",
      "|           back.|   21|\n",
      "|       teardrop.|   15|\n",
      "|        ipsweep.|   13|\n",
      "|      portsweep.|   12|\n",
      "|          satan.|    9|\n",
      "|    warezclient.|    7|\n",
      "|            pod.|    2|\n",
      "|           nmap.|    2|\n",
      "|buffer_overflow.|    2|\n",
      "|   guess_passwd.|    1|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anomalies.select(\"cluster\", \"label\").groupBy('label') \\\n",
    "    .count().orderBy(\"count\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark]",
   "language": "python",
   "name": "conda-env-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
